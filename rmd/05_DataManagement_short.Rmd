---
title: 'Data Managment: Objects and Files'
author: "Thomas Manke"
date: "`r date()`"
output:
  html_document:
    df_print: paged
    code_folding: show
---

# Saving objects as Rdata files
In the process of data exploration we often generate many new data objects. Just as with figures, we may also save data objects for later use. 

The following call saves an "image" of all data obejcts in a compressed file format (conventional: *.Rdata)
```{r, eval=FALSE}
save.image(file="04.Rdata") 
```

**Task**: What happens if the "file=" parameter is omitted? Saving all data can be an overkill. Find out how to save only selected or single objects, e.g. S.

# Saving objects as text files
Rdata files are most convenient for future processing with R.
Sometimes we need to save objects in (tab-separated) text files:
```{r, eval=FALSE}
write.table(iris, file="iris.tsv", sep="\t", row.names=FALSE, quote=FALSE)
```

**Task**: Save only the setosa data in a file setosa.tsv

# Tracking objects
In RStudio, explore the "Environment" tab. On the console there are also many convenient commands to track (and delte) objects from memory.
```{r, eval=FALSE}
ls()     # list all obejct
rm(x)    # remove object x
# rm(list=ls()) # Careful: This removes all objects
```

# Navigation: Where am I, where are my files?
In Rstudio, explore the "Files" tab. On the console:
```{r, eval= FALSE}
getwd()         # get the full path of the current working directory
list.files()    # list all files in working directory
setwd("~")      # change working directory to new directory (here: "~" = home)
list.files(pattern=".Rdata") # list all ".Rdata files in working directory  
```

# some jargon
```{r jargon}
( wd = getwd() )                        # working directory --> full path
( fn = paste(wd,"test.txt", sep="/") ) # filename (may not exist)
( dn = dirname(fn) )                    # directory name 
( bn = basename(fn) )                   # basename
( rp = "../data/test.tsv" )             # relative path  = relative to current work directory
```

# The theory of I/O
Reading external data into R is easy, but it is crucial to have a very clean and well-defined data structure. 
```{r, eval=TRUE}
# load("04.Rdata")  # easiest and fastest for previously generated Rdata files
iris_f=read.table(file="../data/iris.tsv", sep="\t", header=TRUE) 

# Checks
str(iris_f)
head(iris_f)
identical(iris, iris_f)  # sanity check
```



# The reality of I/O
In reality, many external data is badly formatted, leading to much wasted time - before any analysis. 


### Mock real
The code snippet below are not run
```{r trial_and_error, eval=FALSE, error=TRUE}
file="../data/GeneList.tsv"     # if github directory is locally available, find proper relative directory
# alternatively: locate at github https://github.com/maxplanck-ie/Rintro
# file="https://raw.githubusercontent.com/maxplanck-ie/Rintro/master/data/GeneList.tsv"  
read.table(file) 
read.table(file, header=TRUE)                                            # watch out for headers
read.table(file, header=TRUE, comment.char = "%")                        # comment lines
read.table(file, header=TRUE, comment.char = "%", sep="\t")              # separators
read.table(file, header=TRUE, comment.char = "%", sep="\t", quote="\"")  # quote character. What is the default?
```

Happy with results? Fill the data frame
```{r full_read}
file="../data/GeneList.tsv" 
D=read.table(file, header=TRUE, comment.char = "%", sep="\t", quote="\"")
```

Many more alternatives:
```{r alt, eval=FALSE}
file="../data/GeneList.tsv"
read.delim(file, comment.char = "%")      # certain useful (default) parameters in place

# other packages:
readr::read_delim(file, delim ="\t", comment = "%")

# even from EXCEL
file="../data/MouseFoodExperiment.xlsx"
readxl::read_excel(file)
```
Also see: "File->Import Dataset"


```{r checks}
str(D)
is.na(D)  # any NA's?

colSums(is.na(D))
i=rowSums(is.na(D[,3:5]))==0   # keep rows without NA
j=colSums(is.na(D))==nrow(D)   # remove columns with all NA
D[i,!j]
```

### Real real (Homework)
Below is a real world example to get data from one of the first large scale ChIP-studies (Rick Young lab).
This data forms the basis of a data exploration exercise (later), but here we focus only on data retrieval.

Notice the individual and complex filtering process. Nothing of this will be transferable to other data sets.

```{r, eval=FALSE}
# consider a data file from one of the first large scale ChIP-studies (Rick Young lab: yeast)
# 1. Find their homepage (google) and go to the "Data Download"" section.
# 2. find the Publication by Lee et al. (2002) Transcriptional Regulatory Networks in Saccharomyces cerevisea.
# 3. In the "Download Raw Data" section copy the second link ("binding of regulators to genes (text file)"
# this file contains binding data (p-values and ratios) for more than 6000 yeast genes and 113 yeast transcription factors

chip="http://jura.wi.mit.edu/young_public/regulatory_network/binding_by_gene.tsv" # R can read http. 

# Not advisable if network is slow, or repeated trial/errors are to be expected. In this case download first.

# first problem: two header lines --> skip first --> skip line 1
D=read.table(file=chip, header=TRUE,  skip=1, sep="\t")
nrow(D)  # 511 << 6000 !!!???

# second problem: quote symbols in names (')
cf=count.fields(chip,sep="\t") # many NA --> quoted fields got missing

D=read.table(chip, header=TRUE,  sep="\t", quote="\"", skip=1)
rownames(D)=D[,1]          # ORF-symbols in column 1, keep them as rownames 
ic=c(1:4,seq(6,230,by=2))  # define column indices to be excluded (only keep p-values)
P=D[,-ic]                  # new data-frame of p-values, exclude superfluous columns and ratios

write.table(P,file="../data/chip.tsv", sep="\t", quote=FALSE)  # write p-values to clean file
```

***

# Review:

* exporting objects as Rdata (save.image) or text-file (write.table)
* navigate in directories, know where you are working: ls
* I/O pain: keep data clean and well structured !

